## **TensorFlow RDMA 源码剖析——后传** 

## **前言**

在“前传”中，我们介绍了TensorFlow如何利用gRPC完成RDMA的初始化，在“中传”中，我们介绍了TensorFlow如何利用RDMA进行通信。TensorFlow RDMA模块其实是对原有的TensorFlow gRPC包裹了一层，RDMA的初始化模块继承于gRPC模块，VerbsServer是GrpcServer的派生类，VerbsServer在完成常规的GrpcServer操作之后，对RDMA进行初始化。RDMA通信则是对Send节点到Recv节点通信进行重写。在这一节，作为“TensorFlow RDMA 源码剖析”系列的最后一个部分，我们回归到TensorFlow的本质，从计算图中Send和Recv节点的角度出发，剖析TensorFlow是如何实现分布式计算图间的通信的。

## **SendOp和RecvOp具体实现** 

TensorFlow的核心就是计算流图，我们这个系列的主人公就是SendOp和RecvOp这两个算子（节点）。SendOp和RecvOp定义在sendrecv_ops.h中，与其它几百个算子一样，它们的实现都位于tensorflow/core/kernels/文件夹下。与其它几百个算子一样，SendOp和RecvOp的具体实现就是Compute（）函数，它定义了send和recv算子在计算图中的行为。SendOp和RecvOp在分布式训练中至关重要，因为它们定制了单机多卡/多机多卡的计算图之间如何完成tensor通信。
![pic8](http://git.code.oa.com/paichen/TensorFlow-source/raw/master/TensorFlow%20RDMA/pic/pic1.png)
### **1、SendOp** 

```cpp
class SendOp : public OpKernel {
 public:
  explicit SendOp(OpKernelConstruction* ctx);
  void Compute(OpKernelContext* ctx) override;
 private:
  string key_prefix_;
  TF_DISALLOW_COPY_AND_ASSIGN(SendOp);
};

class RecvOp : public AsyncOpKernel {
 public:
  explicit RecvOp(OpKernelConstruction* ctx);
  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override;
 private:
  string key_prefix_;
  TF_DISALLOW_COPY_AND_ASSIGN(RecvOp);
};
```

SendOp::Compute是Send算子的具体实现，它的核心就是调用rendezvous()的Send函数发送tensor，这个Send函数是虚函数，它调用的是Rendezvous指针指向的实例，然而，不管Rendezvous指针指向谁，最终都是调用LocalRendezvousImpl的send（），也就是说，不管是单机还是分布式（gRPC or RDMA），Send函数都是一样的，就是把tensor放在本地，被动的等待消费者（RecvOp）发出请求。详细的调用栈见下图。

```cpp
void SendOp::Compute(OpKernelContext* ctx) {
  OP_REQUIRES(
      ctx, ctx->rendezvous() != nullptr,
      errors::Internal("Op kernel context needs to provide a rendezvous."));
  Rendezvous::ParsedKey parsed;
  GetRendezvousKey(key_prefix_, ctx->frame_iter(), &parsed.buf_);
  VLOG(2) << "Send " << parsed.buf_;
  OP_REQUIRES_OK(ctx, Rendezvous::ParseKey(parsed.buf_, &parsed));
  // The device context may be passed between the Send/Recv
  // boundary, so that the device context used to produce the Tensor
  // is used when performing the copy on the recv side (which may be
  // a different device).
  Rendezvous::Args args;
  args.device_context = ctx->op_device_context();
  args.alloc_attrs = ctx->input_alloc_attr(0);
  OP_REQUIRES_OK(ctx, ctx->rendezvous()->Send(parsed, args, ctx->input(0),
                                              ctx->is_input_dead()));
}
```

![pic8](http://git.code.oa.com/paichen/TensorFlow-source/raw/master/TensorFlow%20RDMA/pic/pic8.png)

### **2、RecvOp** 

RecvOp::ComputeAsync是Recv算子的具体实现，它的核心就是调用rendezvous()->RecvAsync向生产者（SendOp）请求tensor，RecvAsync是一个虚函数，也就是说，对于不同的派生类对象（本地、gPRC、RDMA）RecvAsync的实现是不同的，详细的调用栈建下图。

```cpp
void RecvOp::ComputeAsync(OpKernelContext* ctx, DoneCallback done) {
  OP_REQUIRES(
      ctx, ctx->rendezvous() != nullptr,
      errors::Internal("Op kernel context needs to provide a rendezvous."));
  Rendezvous::ParsedKey parsed;
  GetRendezvousKey(key_prefix_, ctx->frame_iter(), &parsed.buf_);
  ......
  Rendezvous::DoneCallback done_cb = std::bind(
      [ctx](DoneCallback done,
            // Begin unbound arguments.
            const Status& s, const Rendezvous::Args& send_args,
            const Rendezvous::Args& recv_args, const Tensor& val,
            bool is_dead) {
        ctx->SetStatus(s);
        if (s.ok()) {
          // 'ctx' allocates the output tensor of the expected type.
          // The runtime checks whether the tensor received here is
          // the same type.
          if (!is_dead) {
            ctx->set_output(0, val);
          }
          *ctx->is_output_dead() = is_dead;
        }
        done();
      },
      std::move(done), _1, _2, _3, _4, _5);
  ctx->rendezvous()->RecvAsync(parsed, args, std::move(done_cb));
}
```

![pic1](http://git.code.oa.com/paichen/TensorFlow-source/raw/master/TensorFlow%20RDMA/pic/pic7.png)

RecvOp最终调的是BaseRemoteRendezvous中实现的RecvAsync，而RecvAsync会根据tensor传输的src和dst位置决定调用哪种实现，当src和dst位于一个节点时，RecvAsync会调用LocalRendezvousImpl的RecvAsync方法完成本地Recv操作。当src和dst位于不同节点，RecvAsync会调用RecvFromRemoteAsync完成远程Recv操作，RecvFromRemoteAsync是一个接口函数，它的具体实现取决于用户选择使用哪种通信方式（gRPC / RDMA），当用gRPC通信时，调用RpcRemoteRendezvous中的RecvFromRemoteAsync实现。当用RDMA通信时，调用RdmaRemoteRendezvous中的RecvFromRemoteAsync实现。怎么样？ 看到RdmaRemoteRendezvous::RecvFromRemoteAsync是不是很亲切，这就是我们在“中传”中详细介绍的使用RDMA完成recv算子向send算子请求tensor的具体实现呀！！

```cpp
void BaseRemoteRendezvous::RecvAsync(const ParsedKey& parsed,
                                     const Rendezvous::Args& recv_args,
                                     DoneCallback done) {
  ......
  // Are src and dst in the same worker?
  if (IsSameWorker(parsed.src, parsed.dst)) {
    // Recv the tensor from local_.
    local_->RecvAsync(
        parsed, recv_args,
        [this, parsed, done](
            const Status& status, const Rendezvous::Args& send_args,
            const Rendezvous::Args& recv_args, const Tensor& in, bool is_dead) {
          ......
        });
    return;
  } else {
    RecvFromRemoteAsync(parsed, recv_args, std::move(done));
  }
}
```

至此，TensorFlow计算流图中的Send节点和Recv节点是如何完成通信的我们就剖析完了。总的来说就是，对于单机和分布式，Send节点的操作大体相同，都是把计算好的tensor放在本地，被动的等待Recv节点（本地/远程）发出请求。而Recv节点会根据需要通信的两个节点是否处于同一个物理节点以及通信模式采用gRPC还是效率更高的RDMA来决定最终调用那个实现。如果用户选择了RDMA方式通信，那么调用的就是我们“前传”和“中传” 中介绍的通信方法了。
